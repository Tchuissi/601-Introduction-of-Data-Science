{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Week 4: Assignment#1\n",
    "\n",
    "Attached Files:\n",
    "File xml_containing_html.xml.zip (47.042 KB)\n",
    "I provide you with a compressed XML file. Some of the fields contain HTML. ​\n",
    "\n",
    "1.  Extract the XML from the .zip file. In Python, use a module to parse the XML (do not write an XML parser)​\n",
    "2.  Using Python, extract the HTML from the XML. Then use BeautifulSoup4 to parse the HTML \n",
    "3.  For each HTML page, report the number of links (URLs with the tag < a href=\"URL\">text) in each HTML page \n",
    "4.  Submit a single Jupyter notebook that parses the XML file and produces the count of links per HTML file.​\n",
    "\n",
    "Advanced students: if you complete the assignment above and are are seeking a challenge, use an alternative method (i.e., regular expressions or Python's find) to validate the count of HTML links per page reported by BeautifulSoup4 .​ ​\n",
    "\n",
    "When you are done with the assignment or have spent an hour on the homework (whichever comes first), please send me an email indicating which of these you have reached​ ​\n",
    "\n",
    "    To: benpayne@umbc.edu​\n",
    "    Subject: Data 601 Spring 2020 Week 3 HTML in XML\n",
    "    Examples of content: \"I have spent 1 hour on the homework\" or \"I am done with the assignment in 20 minutes.\"​\n",
    "    \n",
    "If you've not completed the homework, please specify what your status is.​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The plan:\n",
    "\n",
    "First, I plan to review the week 3 Powerpoint slides.  Then I plan to review the XML video from week 1 (per your suggestion).  Then I plan to parse the XML file to get a dictionary.  I'm going try to use the parser from the week one video.  I then plan to use BeautifulSoup4 to parse the HTML data.  The result from that parse should yield both individual HTML pages, and the links on those pages.  A loop should allow me to loop through each HTML page, and count the number of links within each page.  The trick will be how do I go from one HTML page to another.  Should be fun.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated plan:\n",
    "\n",
    "After much hard work any many rabbit holes, I decided to put the appropriate cells following this markdown.  Below those cells will be a markdown cell labeled, \"Trials and Tribulations,\" so you can see what I went through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in c:\\users\\vincent\\anaconda3\\lib\\site-packages (0.12.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import into Jupyter notebooks the libraries I think I'll need, and install 'xmltodict,' onto my laptop in Python\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas\n",
    "!pip install xmltodict\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'xmltodict,' into Jupyter notebooks\n",
    "\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "collections.OrderedDict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the 'xmltodict,'' create 'my_doc,' and find out what 'my_doc,' is.\n",
    "\n",
    "with open('xml_containing_html.xml') as fd:\n",
    "    my_doc = xmltodict.parse(fd.read())\n",
    "type(my_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the json library into Jupyter notebooks.\n",
    "# Using the collections library, import OrderedDict into Jupyter notebooks\n",
    "# Create the ordered dictionary, 'output_dict'\n",
    "# Verify what it is.\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "output_dict = json.loads(json.dumps(my_doc))\n",
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['all_pages'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since it's a dictionary, it has keys, and the keys contain data.  So lets look at the keys.\n",
    "\n",
    "output_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So let's see what 'all_pages,' is\n",
    "\n",
    "type(output_dict['all_pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dvtahsakkl', 'cgrjdytyzi', 'ehjctahvdf', 'gegdicoysk', 'oxvhivzrwj', 'apfomcmbhj', 'fupishfoqa', 'iauhlzazja', 'gdgwawqatz', 'zxrcissymw', 'rdbhohsflv', 'vxmqiwlbqe', 'udeqcsiszr', 'qwpkylrbrr', 'lymzvfywqq', 'zyqxunkrxa', 'uysxciwofn', 'igotvwkqea', 'lkdmucpxoy', 'efrussmqjn', 'vqfysayelb', 'gbrldyxfez', 'rybedojcty', 'dbirzzdsym', 'nkmvchoewf', 'ucthjwjmsr', 'rqlvdkeirj', 'jpjchvrjuz', 'nmfneaqqqa', 'odcnaagpmw', 'svxswqrgyc', 'nvsslnawpw', 'ybduhcyjvw', 'qxggagatpb', 'fmrqzdkrbl', 'kztilvoogk', 'shyectmblt', 'iepiizvnfm', 'sorvmhvlkl', 'zqbtnswygo', 'gxyhofgpxf', 'mpbludaqaz', 'wuvqaazfzw', 'ebwrucmyrz', 'qgwipnyeak', 'gcfgxflwkw', 'lizdhvjogk', 'lgkaprlcuv', 'xtaojsiyfr', 'yopehcnimk', 'hystkchmuy', 'qhlrlpvpsb', 'wxdfwtzrgw', 'pmroqsiixy', 'udmyaofwle', 'ijxyjjuera', 'sgmjcltjah', 'jzcjszwpzz', 'ednaukjyph', 'opbqmxjwje', 'mlbxjluray', 'irymwkrjsi', 'ysmqqnvwvn', 'ovqriyyuci', 'dvvnqlilox', 'ykwntcsuwt', 'xtnygnhixm', 'myhswkhpki', 'vwkgfkksjm', 'vbbcsdorcg', 'hoiyjycypb', 'vhqhythejg', 'uqhgwzkbom', 'fiaclvnheo', 'pxpwgrcbdd', 'czdxzzooym', 'pmsjrwsevv', 'bbedpkvyab', 'lbuhvscgkk', 'oxjhfmimnt', 'eiorepgjis', 'fjpljuyaxt', 'cpsrqeeeea', 'zktkruwtou', 'bgpueyqska', 'dqhcnfibka', 'sfiswolxni', 'ifyjbzdoso', 'wedcqhnkln', 'ffxlwheqmp', 'riwugfsklb', 'xzckhnuznp', 'jbcvekrbbz', 'vyoeghcijo', 'ynpbdvrejd', 'rcnavmughq', 'acbvizjzzx', 'sqeijbwjcl', 'xjenwlhhtn', 'rhiknwowqf', 'pfuznvvzrr', 'strgwhakcq', 'eqeriaqwdi', 'jxhxbmbhth', 'lbbalvlqgk', 'enhtapuawd', 'dssixhxrgn', 'qgoyyqkbhk', 'hrybxxpker', 'otkmsxltkj', 'lobghhizyn', 'nlftedyyxe', 'gvtdsymfyt', 'sfssvquect', 'sfbrwgaxsy', 'ntyeqdgjix', 'sojgtpybkd', 'khaybegcez', 'mgzoydtbca', 'nicqwbqqte', 'mvypwnaesk', 'qkiddtjpyr', 'mnrllbwmgf', 'dhbyzdthya', 'ddkgzxnkgs'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, since it's a dictionary, it has keys, and the keys contain data.  So lets look at the keys.\n",
    "\n",
    "output_dict['all_pages'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we're getting somwhere.  The keys contain all the website data including the HTML. \n",
    "# So lets look at the first one to begin the development for my loop.\n",
    "\n",
    "type(output_dict['all_pages']['dvtahsakkl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['@author', 'content', 'source', 'date'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another dictionary, lets look the key.\n",
    "\n",
    "output_dict['all_pages']['dvtahsakkl'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<HTML>\\n<TITLE>observations on the role of IT trainer</TITLE>\\n<BODY>\\n<P>It appears that the natural general principle that will subsume this\\ncase is, apparently, determined by a stipulation <a href=\"https://www.munoz-peterson.com/\">to</a> place the\\nconstructions in<a href=\"https://www.munoz-peterson.com/\">to</a> these various categories. For any transformation\\nwhich is sufficiently diversified in application to be of any interest,\\nthe earlier discussion of deviance suffices to account for irrelevant\\nintervening contexts in selectional rules. We will bring evidence in\\nfavor of the following thesis: this analysis of a formative as a pair of\\nsets of features is not subject to the requirement that branching is not\\ntolerated within the dominance scope of a complex symbol.</P>\\n<ol>\\n<LI>Grass-roots coherent orchestration</LI>\\n<LI>Compatible encompassing paradigm</LI>\\n<LI>Digitized zero administration matrices</LI>\\n<LI>User-friendly attitude-oriented productivity</LI>\\n<LI>Polarized clear-thinking productivity</LI>\\n</ol>\\n<img src=\"https://www.lorempixel.com/965/749\">\\n<P>To provide a constituent structure for T(Z,K), the descriptive power of\\nthe base component delimits a parasitic gap construction. Nevertheless,\\nthis analysis of a formative as a pair of sets of features is,\\napparently, determined by a stipulation to place the constructions into\\nthese various categories. However, this assumption is not correct, since\\nthe natural general principle that will subsume this case is not quite\\nequivalent to a general convention regarding the forms of the grammar.\\nFor one thing, the earlier discussion of deviance is not subject to a\\ndescriptive <a href=\"https://richardson.net/\">fact.</P>\\n</a><ol>\\n<LI>Operative bifurcated concept</LI>\\n<LI>User-friendly optimizing definition</LI>\\n<LI>Synergized local architecture</LI>\\n<LI>Organized systemic encoding</LI>\\n</ol>\\n<img src=\"https://placekitten.com/513/198\">\\n<P>Let us continue to suppose that any associated supporting element cannot\\nbe arbitrary in the levels <a href=\"https://smith.com/\">of</a> acceptability from fairly high (e.g.\\n(99a)) to virtual gibberish (e.g. (98d)). If the position <a href=\"https://smith.com/\">of</a> the trace\\nin (99c) were only relatively inaccessible to movement, a case of\\nsemigrammaticalness of a different sort is not subject to the\\ntraditional practice of grammarians. Conversely, an important property\\nof these three types of EC is not quite equivalent to problems of\\nphonemic and morphological analysis. A consequence of the approach just\\noutlined is that the natural general principle that will subsume this\\ncase is unspecified with respect to nondistinctness in the sense of\\ndistinctive feature theory.</P>\\n\\n</BODY>\\n</HTML>'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So I've looked at the XML, and I can tell that the, 'content,' contains the HTML data.\n",
    "# Let's verify\n",
    "\n",
    "output_dict['all_pages']['dvtahsakkl']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HTML>\n",
      "<TITLE>observations on the role of IT trainer</TITLE>\n",
      "<BODY>\n",
      "<P>It appears that the natural general principle that will subsume this\n",
      "case is, apparently, determined by a stipulation <a href=\"https://www.munoz-peterson.com/\">to</a> place the\n",
      "constructions in<a href=\"https://www.munoz-peterson.com/\">to</a> these various categories. For any transformation\n",
      "which is sufficiently diversified in application to be of any interest,\n",
      "the earlier discussion of deviance suffices to account for irrelevant\n",
      "intervening contexts in selectional rules. We will bring evidence in\n",
      "favor of the following thesis: this analysis of a formative as a pair of\n",
      "sets of features is not subject to the requirement that branching is not\n",
      "tolerated within the dominance scope of a complex symbol.</P>\n",
      "<ol>\n",
      "<LI>Grass-roots coherent orchestration</LI>\n",
      "<LI>Compatible encompassing paradigm</LI>\n",
      "<LI>Digitized zero administration matrices</LI>\n",
      "<LI>User-friendly attitude-oriented productivity</LI>\n",
      "<LI>Polarized clear-thinking productivity</LI>\n",
      "</ol>\n",
      "<img src=\"https://www.lorempixel.com/965/749\">\n",
      "<P>To provide a constituent structure for T(Z,K), the descriptive power of\n",
      "the base component delimits a parasitic gap construction. Nevertheless,\n",
      "this analysis of a formative as a pair of sets of features is,\n",
      "apparently, determined by a stipulation to place the constructions into\n",
      "these various categories. However, this assumption is not correct, since\n",
      "the natural general principle that will subsume this case is not quite\n",
      "equivalent to a general convention regarding the forms of the grammar.\n",
      "For one thing, the earlier discussion of deviance is not subject to a\n",
      "descriptive <a href=\"https://richardson.net/\">fact.</P>\n",
      "</a><ol>\n",
      "<LI>Operative bifurcated concept</LI>\n",
      "<LI>User-friendly optimizing definition</LI>\n",
      "<LI>Synergized local architecture</LI>\n",
      "<LI>Organized systemic encoding</LI>\n",
      "</ol>\n",
      "<img src=\"https://placekitten.com/513/198\">\n",
      "<P>Let us continue to suppose that any associated supporting element cannot\n",
      "be arbitrary in the levels <a href=\"https://smith.com/\">of</a> acceptability from fairly high (e.g.\n",
      "(99a)) to virtual gibberish (e.g. (98d)). If the position <a href=\"https://smith.com/\">of</a> the trace\n",
      "in (99c) were only relatively inaccessible to movement, a case of\n",
      "semigrammaticalness of a different sort is not subject to the\n",
      "traditional practice of grammarians. Conversely, an important property\n",
      "of these three types of EC is not quite equivalent to problems of\n",
      "phonemic and morphological analysis. A consequence of the approach just\n",
      "outlined is that the natural general principle that will subsume this\n",
      "case is unspecified with respect to nondistinctness in the sense of\n",
      "distinctive feature theory.</P>\n",
      "\n",
      "</BODY>\n",
      "</HTML>\n"
     ]
    }
   ],
   "source": [
    "# Viola! I'm now going to create the variable, 'html_content,' and load the contents of, 'content,' into it.\n",
    "# I'm printing out just to verify.\n",
    "\n",
    "html_content = output_dict['all_pages']['dvtahsakkl']['content']\n",
    "print(html_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>observations on the role of IT trainer</title>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verified.  Now we run BeautifulSoup to parse out the HTML.\n",
    "# Verify it works.\n",
    "\n",
    "soup = BeautifulSoup(output_dict['all_pages']['dvtahsakkl']['content'], 'html.parser')\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.munoz-peterson.com/\n",
      "https://www.munoz-peterson.com/\n",
      "https://richardson.net/\n",
      "https://smith.com/\n",
      "https://smith.com/\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Verified! Make a loop to find and count all the links on this webpage.\n",
    "\n",
    "count = 0\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "    count = count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of links in, Reed is =  5\n",
      "The number of links in, Dunlap is =  5\n",
      "The number of links in, Evans is =  4\n",
      "The number of links in, Nielsen is =  5\n",
      "The number of links in, Hansen is =  5\n",
      "The number of links in, Ramirez is =  4\n",
      "The number of links in, Fowler is =  4\n",
      "The number of links in, Brown is =  3\n",
      "The number of links in, Li is =  3\n",
      "The number of links in, Marshall is =  4\n",
      "The number of links in, Finley is =  6\n",
      "The number of links in, Marshall is =  3\n",
      "The number of links in, King is =  5\n",
      "The number of links in, Mitchell is =  5\n",
      "The number of links in, Gentry is =  4\n",
      "The number of links in, Rivera is =  4\n",
      "The number of links in, Johnson is =  5\n",
      "The number of links in, Wolfe is =  5\n",
      "The number of links in, Wheeler is =  3\n",
      "The number of links in, Keller is =  3\n",
      "The number of links in, Singh is =  4\n",
      "The number of links in, Garcia is =  3\n",
      "The number of links in, Brown is =  3\n",
      "The number of links in, Jennings is =  4\n",
      "The number of links in, Lee is =  4\n",
      "The number of links in, Brown is =  3\n",
      "The number of links in, Gordon is =  5\n",
      "The number of links in, Stokes is =  4\n",
      "The number of links in, Gonzalez is =  4\n",
      "The number of links in, York is =  3\n",
      "The number of links in, Thomas is =  3\n",
      "The number of links in, Munoz is =  5\n",
      "The number of links in, Robinson is =  4\n",
      "The number of links in, Price is =  3\n",
      "The number of links in, Frye is =  5\n",
      "The number of links in, Mckinney is =  5\n",
      "The number of links in, Smith is =  4\n",
      "The number of links in, Perez is =  5\n",
      "The number of links in, Miller is =  4\n",
      "The number of links in, Harris is =  4\n",
      "The number of links in, Mendoza is =  4\n",
      "The number of links in, Turner is =  3\n",
      "The number of links in, Lambert is =  6\n",
      "The number of links in, Brown is =  6\n",
      "The number of links in, Walker is =  4\n",
      "The number of links in, Robinson is =  3\n",
      "The number of links in, Bryant is =  4\n",
      "The number of links in, Blackwell is =  4\n",
      "The number of links in, Holmes is =  4\n",
      "The number of links in, Maldonado is =  3\n",
      "The number of links in, Martin is =  4\n",
      "The number of links in, Anderson is =  4\n",
      "The number of links in, Cole is =  5\n",
      "The number of links in, Reyes is =  3\n",
      "The number of links in, Galvan is =  4\n",
      "The number of links in, Maynard is =  4\n",
      "The number of links in, Edwards is =  3\n",
      "The number of links in, Young is =  5\n",
      "The number of links in, King is =  6\n",
      "The number of links in, Gallagher is =  3\n",
      "The number of links in, Cabrera is =  3\n",
      "The number of links in, Cole is =  4\n",
      "The number of links in, Sawyer is =  5\n",
      "The number of links in, Gates is =  4\n",
      "The number of links in, Reyes is =  4\n",
      "The number of links in, Dennis is =  3\n",
      "The number of links in, Rodriguez is =  3\n",
      "The number of links in, Lee is =  5\n",
      "The number of links in, Crane is =  6\n",
      "The number of links in, Jordan is =  4\n",
      "The number of links in, Smith is =  4\n",
      "The number of links in, Robertson is =  5\n",
      "The number of links in, Garcia is =  3\n",
      "The number of links in, Holland is =  5\n",
      "The number of links in, Delgado is =  4\n",
      "The number of links in, Mckinney is =  3\n",
      "The number of links in, Collins is =  5\n",
      "The number of links in, Lewis is =  4\n",
      "The number of links in, Davis is =  3\n",
      "The number of links in, Reed is =  3\n",
      "The number of links in, Ruiz is =  3\n",
      "The number of links in, Gaines is =  4\n",
      "The number of links in, Smith is =  3\n",
      "The number of links in, Villarreal is =  4\n",
      "The number of links in, Boyer is =  4\n",
      "The number of links in, Banks is =  3\n",
      "The number of links in, Bowman is =  4\n",
      "The number of links in, Bowers is =  4\n",
      "The number of links in, Fuentes is =  5\n",
      "The number of links in, Hickman is =  5\n",
      "The number of links in, Smith is =  3\n",
      "The number of links in, Powell is =  3\n",
      "The number of links in, Mata is =  5\n",
      "The number of links in, Berry is =  5\n",
      "The number of links in, Bullock is =  5\n",
      "The number of links in, Clark is =  3\n",
      "The number of links in, Davis is =  5\n",
      "The number of links in, Lam is =  5\n",
      "The number of links in, White is =  5\n",
      "The number of links in, Kelly is =  4\n",
      "The number of links in, Pope is =  3\n",
      "The number of links in, Griffin is =  4\n",
      "The number of links in, Cortez is =  3\n",
      "The number of links in, Taylor is =  3\n",
      "The number of links in, Miller is =  5\n",
      "The number of links in, Duran is =  4\n",
      "The number of links in, Cooley is =  3\n",
      "The number of links in, Scott is =  3\n",
      "The number of links in, Sanchez is =  3\n",
      "The number of links in, Carney is =  4\n",
      "The number of links in, Evans is =  4\n",
      "The number of links in, Edwards is =  3\n",
      "The number of links in, Potter is =  5\n",
      "The number of links in, Zimmerman is =  4\n",
      "The number of links in, Singh is =  4\n",
      "The number of links in, Lucas is =  4\n",
      "The number of links in, Sosa is =  5\n",
      "The number of links in, Bowen is =  4\n",
      "The number of links in, Guerrero is =  4\n",
      "The number of links in, Hartman is =  5\n",
      "The number of links in, Bruce is =  3\n",
      "The number of links in, Hamilton is =  3\n",
      "The number of links in, Callahan is =  4\n",
      "The number of links in, Murillo is =  3\n",
      "The number of links in, Gordon is =  4\n"
     ]
    }
   ],
   "source": [
    "# Finally, write the loop to go through all the webpages and count the links.\n",
    "\n",
    "html_content = []\n",
    "for item in output_dict['all_pages']:\n",
    "    html_content = output_dict['all_pages'][item]['content']\n",
    "    soup = BeautifulSoup(output_dict['all_pages'][item]['content'], 'html.parser')\n",
    "    count = 0\n",
    "    for link in soup.find_all('a'):\n",
    "        count = count+1\n",
    "    print('The number of links in,', output_dict['all_pages'][item]['@author'],'is = ', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done.  Code looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some lessons learned:\n",
    "\n",
    "'Installed,' means onto your PC in Python. 'Imported,' means into Jupytor Notebook.\n",
    "\n",
    "A good method to solve a coding problem in data science, is to generate something, see what it is, see what you can get from it, create a variable for the things you find that you'll need, use these variables to solve the problem.\n",
    "\n",
    "It's okay to ask the professor for help.\n",
    "\n",
    "Don't be fraid to start the assignment early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trials and Tribulations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some of the things I went through before I go the above code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Download the libraries I think I'll need\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "#pip install beautifulsoup4\n",
    "import requests\n",
    "import pandas\n",
    "#!conda install -c conda-forge xmltodict\n",
    "#python -m pip3 install xmltodict\n",
    "#conda install -c anaconda xmltodict\n",
    "#get -pip.py\n",
    "#sudo yum install python-xmltodict\n",
    "#sudo pip3 install xmltodict\n",
    "!pip install xmltodict\n",
    "#conda install -c xmltodict\n",
    "#conda install -c conda-forge/label/gcc7 xmltodict\n",
    "#conda install -c conda-forge/label/cf201901 xmltodict\n",
    "pandas.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://anaconda.org/conda-forge/xmltodict\n",
    "\n",
    "https://stackoverflow.com/questions/23919161/importerror-no-module-named-xmltodict-while-calling-from-other-python-script\n",
    "\n",
    "https://stackoverflow.com/questions/36981481/problems-with-python-3-4-and-xmltodict\n",
    "\n",
    "https://stackoverflow.com/questions/21699688/pip-install-xmltodict-not-working"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('xml_containing_html.xml') as fd:\n",
    "    my_doc = xmltodict.parse(fd.read())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(my_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I need to figure out how to extract the HTML from the XML in the dictionary I've created.  I assume there's a way to delete all the dictionary \"values,\" that are not needed.  Oh boy!"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "output_dict = json.loads(json.dumps(my_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't understand what I'm looking at.  I understand this is an unordered dictionary collection.  I can get it to \"show,\" me 'all_pages', but what I want it to do is show me just the 'content.'  Then I can save that to another variable, and run BeautifulSoup on it.  I read all I can about dictionaries, but this doesn't seem to be a dictionary.\n",
    "\n",
    "https://stackoverflow.com/questions/10058140/accessing-items-in-an-collections-ordereddict-by-index\n",
    "\n",
    "https://docs.python.org/3.3/library/stdtypes.html#dict-views"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(output_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here forward is what I did after Ben's latest email.  New strategy.  'all_pages,' is a dictionary. The keys to the dictionary look like a list.  Upon looking at one of the keys, I found out it too is a dictionary, and one of its keys is, 'content.'  Next, I found out that each 'content,' key is a list.  So I think I can do one of two things: a) I can loop through the 'all_pages,' dictionary and extract the, 'content' ('content,' is a string that can be searched) from each key, use the BeatifulSoup to extract the HTML and count the links, or b) convert the 'all_pages,' keys into a list. Loop through the 'all_pages,' dictionary by each key and loop through the 'content,' and use the BeautifulSoup to extract the HTML and count the links."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(output_dict['all_pages'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_dict['all_pages'].keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(output_dict['all_pages']['dvtahsakkl'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_dict['all_pages']['dvtahsakkl'].keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(output_dict['all_pages']['dvtahsakkl']['content'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "output_dict['all_pages']['dvtahsakkl']['content']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "html_content = output_dict['all_pages']['dvtahsakkl']['content']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the following from here:\n",
    "\n",
    "https://stackoverflow.com/questions/21400458/can-we-access-key-and-value-in-the-ordereddict-in-python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key, value in output_dict['all_pages']['dvtahsakkl'].items():\n",
    "#    print (key)\n",
    "#    print (value)\n",
    "    test_file_key = (key)\n",
    "#    test_file_value = (value)\n",
    "    print (test_file_key)\n",
    "#    print (test_file_value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(test_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to strip @author value, and source value, and date value from my test_file.  Back to Google! Found the following:\n",
    "\n",
    "https://stackoverflow.com/questions/27155819/delete-a-key-and-value-from-an-ordereddict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del output_dict['all_pages']['dvtahsakkl']['@author']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key, value in output_dict['all_pages']['dvtahsakkl'].items():\n",
    "#    print (key)\n",
    "#    print (value)\n",
    "    test_file_key = (key)\n",
    "    test_file_value = (value)\n",
    "    print (test_file_key)\n",
    "#    print (test_file_value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del output_dict['all_pages']['dvtahsakkl']['source']\n",
    "del output_dict['all_pages']['dvtahsakkl']['date']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for key, value in output_dict['all_pages']['dvtahsakkl'].items():\n",
    "#    print (key)\n",
    "#    print (value)\n",
    "    test_file_key = (key)\n",
    "    test_file_value = (value)\n",
    "#    print (test_file_key)\n",
    "    print (test_file_value)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup = BeautifulSoup(test_file_value, 'html.parser')\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count = 0\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))\n",
    "    count = count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I did it for a single element in the 'all_pages,' dictionary.  Now, I need to convert the 'all_pages,' keys into a list so I can do all of the webpages.  This should be fun.  Back to Google.\n",
    "\n",
    "https://stackoverflow.com/questions/16819222/how-to-return-dictionary-keys-as-a-list-in-python"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "my_keys_list = list(output_dict['all_pages'].keys())\n",
    "type(my_keys_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(my_keys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy!  Now onto the loop."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_file_key = []\n",
    "test_file_value = []\n",
    "for i in my_keys_list:\n",
    "    for key, value in output_dict['all_pages'][i].items():\n",
    "        test_file_key = (key)\n",
    "        print (test_file_key)\n",
    "#        for a in test_file_key:\n",
    "        del output_dict['all_pages'][i]['@author']\n",
    "        test_file_value = (value)\n",
    "        print (test_file_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
