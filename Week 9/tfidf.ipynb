{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import fnmatch # https://docs.python.org/3/library/fnmatch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source https://stevenloria.com/tf-idf/ <BR>\n",
    "\n",
    "Caveat: this post now uses TextBlob for breaking up the text into words and getting the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, list_of_words_in_document):\n",
    "    \"\"\"\n",
    "    computes \"term frequency\" which is the number of times a word appears in a document, \n",
    "    normalized by dividing by the total number of words in document. \n",
    "    \"\"\"\n",
    "    return list_of_words_in_document.count(term)/(len(list_of_words_in_document)*1.0)\n",
    "\n",
    "def number_of_documents_containing(term,all_documents):\n",
    "    \"\"\"\n",
    "    Returns the number of documents containing word. \n",
    "    \"\"\"\n",
    "    countr=0\n",
    "    for this_doc in all_documents:\n",
    "        if (term in this_doc):\n",
    "            countr+=1\n",
    "    return countr\n",
    "\n",
    "def inverse_doc_freq(term, all_documents):\n",
    "    \"\"\"\n",
    "    computes \"inverse document frequency\" which measures \n",
    "    how common a word is among all documents in corpus. \n",
    "    The more common a word is, the lower its idf. \n",
    "    Take the ratio of the total number of documents to the number of documents containing word, \n",
    "    then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "    \"\"\"\n",
    "    return math.log(len(all_documents) / ( 1.0 + number_of_documents_containing(term, all_documents)))\n",
    "\n",
    "def tfidf(term, list_of_words_in_document, all_documents):\n",
    "    \"\"\"\n",
    "    computes the TF-IDF score. It's the product of tf and idf.\n",
    "    \"\"\"\n",
    "    return term_freq(term, list_of_words_in_document) * inverse_doc_freq(term, all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\*.dat files in the directory have only key words from each file\n",
    "\n",
    "Convert the .dat contents to lists per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: python, TF-IDF: 0.01662\n",
      "\tWord: films, TF-IDF: 0.00997\n",
      "\tWord: made-for-TV, TF-IDF: 0.00665\n",
      "Top words in document 2\n",
      "\tWord: genus, TF-IDF: 0.02192\n",
      "\tWord: 2, TF-IDF: 0.02192\n",
      "\tWord: from, TF-IDF: 0.01096\n",
      "Top words in document 3\n",
      "\tWord: Colt, TF-IDF: 0.01367\n",
      "\tWord: Magnum, TF-IDF: 0.01367\n",
      "\tWord: revolver, TF-IDF: 0.01367\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)\n",
    "\n",
    "document1 = tb(\"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen. The film concerns a genetically engineered snake, a python, that\n",
    "escapes and unleashes itself on a small town. It includes the classic final\n",
    "girl scenario evident in films like Friday the 13th. It was filmed in Los Angeles,\n",
    " California and Malibu, California. Python was followed by two sequels: Python\n",
    " II (2002) and Boa vs. Python (2004), both also made-for-TV films.\"\"\")\n",
    "\n",
    "document2 = tb(\"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\")\n",
    "\n",
    "document3 = tb(\"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment. Some firearm\n",
    "collectors and writers such as Jeff Cooper, Ian V. Hogg, Chuck Hawks, Leroy\n",
    "Thompson, Renee Smeets and Martin Dougherty have described the Python as the\n",
    "finest production revolver ever made.\"\"\")\n",
    "\n",
    "bloblist = [document1, document2, document3]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    print(\"Top words in document {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, blob, bloblist) for word in blob.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_documents={}\n",
    "all_words_from_all_docs=[]\n",
    "all_terms=[]\n",
    "foldr='../week_03_getting_data/essays/'\n",
    "fname='*.dat'\n",
    "for file_name in os.listdir(foldr):\n",
    "    #print(file_name)\n",
    "    if fnmatch.fnmatch(file_name, fname): # Unix shell-style wildcards\n",
    "        print(file_name)\n",
    "        with open(foldr+file_name,'r') as fil:\n",
    "            words_in_file=fil.read().split(\"\\n\")\n",
    "        # remove empty strings from list of words\n",
    "        while \"\" in words_in_file:\n",
    "            words_in_file.remove(\"\")\n",
    "        # save the words per file as value in a dictionary\n",
    "        all_documents[file_name]=words_in_file\n",
    "        print('has',len(words_in_file),'words\\n')\n",
    "        # also save all the words to a list\n",
    "        for this_word in words_in_file:\n",
    "            all_words_from_all_docs.append(this_word)\n",
    "            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_words_from_all_docs = list(set(all_words_from_all_docs))\n",
    "len(all_words_from_all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample sizes are small, so results are not reliable representations of the document"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for doc_name, word_list_in_this_doc in all_documents.items():\n",
    "    if (len(word_list_in_this_doc)==0):\n",
    "        print(\"error: empty input file\"+doc_name)\n",
    "    else:\n",
    "        dic_of_terms={}\n",
    "        for this_term in word_list_in_this_doc:\n",
    "            dic_of_terms[this_term] = tfidf(this_term, word_list_in_this_doc, all_words_from_all_docs)\n",
    "        #print(dic_of_terms)\n",
    "        print('\\n'+doc_name)\n",
    "        terms_in_doc_sorted_by_score=sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "        # first 40 words by importance\n",
    "        for this_tup in terms_in_doc_sorted_by_score[0:40]:\n",
    "            print(this_tup)\n",
    "#        for indx in range(10):\n",
    "#            print(terms_in_doc_sorted_by_score[indx][0] + \":\"+str(terms_in_doc_sorted_by_score[indx][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
