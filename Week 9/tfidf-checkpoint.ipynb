{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import fnmatch # https://docs.python.org/3/library/fnmatch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source https://stevenloria.com/tf-idf/ <BR>\n",
    "\n",
    "Caveat: this post now uses TextBlob for breaking up the text into words and getting the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, list_of_words_in_document):\n",
    "    \"\"\"\n",
    "    computes \"term frequency\" which is the number of times a word appears in a document, \n",
    "    normalized by dividing by the total number of words in document. \n",
    "    \"\"\"\n",
    "    return list_of_words_in_document.count(term)/(len(list_of_words_in_document)*1.0)\n",
    "\n",
    "def number_of_documents_containing(term,all_documents):\n",
    "    \"\"\"\n",
    "    Returns the number of documents containing word. \n",
    "    \"\"\"\n",
    "    countr=0\n",
    "    for this_doc in all_documents:\n",
    "        if (term in this_doc):\n",
    "            countr+=1\n",
    "    return countr\n",
    "\n",
    "def inverse_doc_freq(term, all_documents):\n",
    "    \"\"\"\n",
    "    computes \"inverse document frequency\" which measures \n",
    "    how common a word is among all documents in corpus. \n",
    "    The more common a word is, the lower its idf. \n",
    "    Take the ratio of the total number of documents to the number of documents containing word, \n",
    "    then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "    \"\"\"\n",
    "    return math.log(len(all_documents) / ( 1.0 + number_of_documents_containing(term, all_documents)))\n",
    "\n",
    "def tfidf(term, list_of_words_in_document, all_documents):\n",
    "    \"\"\"\n",
    "    computes the TF-IDF score. It's the product of tf and idf.\n",
    "    \"\"\"\n",
    "    return term_freq(term, list_of_words_in_document) * inverse_doc_freq(term, all_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \\*.dat files in the directory have only key words from each file\n",
    "\n",
    "Convert the .dat contents to lists per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../week_03_getting_data/essays/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4670a130bdcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfoldr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'../week_03_getting_data/essays/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'*.dat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfoldr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m#print(file_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfnmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Unix shell-style wildcards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../week_03_getting_data/essays/'"
     ]
    }
   ],
   "source": [
    "all_documents={}\n",
    "all_words_from_all_docs=[]\n",
    "all_terms=[]\n",
    "foldr='../week_03_getting_data/essays/'\n",
    "fname='*.dat'\n",
    "for file_name in os.listdir(foldr):\n",
    "    #print(file_name)\n",
    "    if fnmatch.fnmatch(file_name, fname): # Unix shell-style wildcards\n",
    "        print(file_name)\n",
    "        with open(foldr+file_name,'r') as fil:\n",
    "            words_in_file=fil.read().split(\"\\n\")\n",
    "        # remove empty strings from list of words\n",
    "        while \"\" in words_in_file:\n",
    "            words_in_file.remove(\"\")\n",
    "        # save the words per file as value in a dictionary\n",
    "        all_documents[file_name]=words_in_file\n",
    "        print('has',len(words_in_file),'words\\n')\n",
    "        # also save all the words to a list\n",
    "        for this_word in words_in_file:\n",
    "            all_words_from_all_docs.append(this_word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_from_all_docs = list(set(all_words_from_all_docs))\n",
    "len(all_words_from_all_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample sizes are small, so results are not reliable representations of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, word_list_in_this_doc in all_documents.items():\n",
    "    if (len(word_list_in_this_doc)==0):\n",
    "        print(\"error: empty input file\"+doc_name)\n",
    "    else:\n",
    "        dic_of_terms={}\n",
    "        for this_term in word_list_in_this_doc:\n",
    "            dic_of_terms[this_term] = tfidf(this_term, word_list_in_this_doc, all_words_from_all_docs)\n",
    "        #print(dic_of_terms)\n",
    "        print('\\n'+doc_name)\n",
    "        terms_in_doc_sorted_by_score=sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "        # first 40 words by importance\n",
    "        for this_tup in terms_in_doc_sorted_by_score[0:40]:\n",
    "            print(this_tup)\n",
    "#        for indx in range(10):\n",
    "#            print(terms_in_doc_sorted_by_score[indx][0] + \":\"+str(terms_in_doc_sorted_by_score[indx][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
